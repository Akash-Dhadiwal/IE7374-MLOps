{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 — Streaming LM Pipeline\n",
    "\n",
    "**Changes from template:**\n",
    "- Dataset: `wikitext` (streaming)\n",
    "- Tokenizer: `gpt2` (GPT-2 tokenizer)\n",
    "- Block size: **256** tokens (was 128)\n",
    "\n",
    "This notebook demonstrates a true streaming data pipeline using Hugging Face Datasets\n",
    "and PyTorch `IterableDataset`. The pipeline tokenizes on the fly, concatenates tokens\n",
    "with a rolling buffer, and yields fixed-length chunks for language model training.\n",
    "\n",
    "Run the cells in order. This notebook is self-contained and intended as the final\n",
    "deliverable for the assignment (ready to submit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Notebook: Streaming LM Pipeline (modified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if necessary (uncomment when needed)\n",
    "!pip3 install datasets transformers torch\n",
    "print(\"Notebook: Streaming LM Pipeline (modified)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashdhadiwal/Downloads/IE7374/IE7374-MLOps/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load the dataset in **streaming** mode\n",
    "\n",
    "We use `wikitext` in streaming mode so that the dataset is not fully loaded into RAM.\n",
    "This mirrors real-world training pipelines where corpora can be very large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_dataset created (streaming=True)\n"
     ]
    }
   ],
   "source": [
    "stream_dataset = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-103-v1\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"stream_dataset created (streaming=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize the tokenizer\n",
    "\n",
    "We use GPT-2's tokenizer. GPT-2 doesn't have a pad token by default, so we set `pad_token` to `eos_token`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initialized: <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer initialized:\", tokenizer.__class__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Tokenization (lazy / streaming)\n",
    "\n",
    "We map tokenization lazily over the streaming dataset. We do not pad/truncate here —\n",
    "we want raw token sequences so they can be concatenated across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization mapping created (lazy, streaming)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # `examples` is a list of dicts when batched=True; HF streaming map provides batched lists\n",
    "    # but in streaming mode it may also provide single examples depending on environment.\n",
    "    # We guard for both cases.\n",
    "    if isinstance(examples, dict) and \"text\" in examples:\n",
    "        texts = examples[\"text\"]\n",
    "    else:\n",
    "        # dataset iterable yields dicts with 'text'\n",
    "        texts = [examples.get(\"text\", \"\")] if isinstance(examples, dict) else examples\n",
    "    return tokenizer(texts)\n",
    "\n",
    "tokenized_stream = stream_dataset.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization mapping created (lazy, streaming)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Rolling buffer to group tokens into fixed-length blocks\n",
    "\n",
    "We use a rolling buffer because streaming datasets are iterators and we cannot look ahead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256  # increased from 128 in the original lab\n",
    "\n",
    "def group_texts_streaming(dataset_iter, block_size):\n",
    "    \"\"\"Generator that yields fixed-length token chunks from a streaming tokenized dataset.\n",
    "    Each yielded item is a dict with 'input_ids' and 'attention_mask'.\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    for example in dataset_iter:\n",
    "        # example may contain 'input_ids' and other fields; we guard for shapes\n",
    "        ids = example.get(\"input_ids\") if isinstance(example, dict) else None\n",
    "        if ids is None:\n",
    "            # if tokenization returned lists under different structure, try to extract\n",
    "            try:\n",
    "                ids = example[\"input_ids\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "        buffer.extend(ids)\n",
    "        while len(buffer) >= block_size:\n",
    "            chunk = buffer[:block_size]\n",
    "            buffer = buffer[block_size:]\n",
    "            yield {\"input_ids\": chunk, \"attention_mask\": [1] * block_size}\n",
    "    # Optionally: yield a final shorter chunk padded to block_size (commented out by default)\n",
    "    # if buffer:\n",
    "    #     pad_len = block_size - len(buffer)\n",
    "    #     yield {\"input_ids\": buffer + [tokenizer.pad_token_id] * pad_len, \"attention_mask\": [1] * len(buffer) + [0] * pad_len}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Wrap generator in a PyTorch `IterableDataset`\n",
    "\n",
    "This allows us to use PyTorch's `DataLoader` with an iterable dataset and a custom collate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped grouped generator into StreamingLMIterableDataset\n"
     ]
    }
   ],
   "source": [
    "class StreamingLMIterableDataset(IterableDataset):\n",
    "    def __init__(self, hf_iterable_dataset, block_size):\n",
    "        self.dataset = hf_iterable_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # The HF iterable returns examples lazily; pass it to the grouping generator\n",
    "        return group_texts_streaming(self.dataset, self.block_size)\n",
    "\n",
    "grouped_iterable_dataset = StreamingLMIterableDataset(tokenized_stream, block_size)\n",
    "print(\"Wrapped grouped generator into StreamingLMIterableDataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Collate function and DataLoader\n",
    "\n",
    "We produce tensors for `input_ids`, `attention_mask`, and `labels` (copy of `input_ids`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created (streaming, batch_size=8)\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": input_ids.clone()\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(grouped_iterable_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "print(\"DataLoader created (streaming, batch_size=8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Inspect a few streaming batches and measure throughput\n",
    "\n",
    "We iterate a few batches and report shapes and throughput (examples/sec).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 -> input_ids shape: torch.Size([8, 256])\n",
      "Batch 1 -> input_ids shape: torch.Size([8, 256])\n",
      "Batch 2 -> input_ids shape: torch.Size([8, 256])\n",
      "Elapsed: 1.42s — approx processed tokens: 6144 — tokens/sec: 4321.74\n"
     ]
    }
   ],
   "source": [
    "num_batches = 3\n",
    "t0 = time.time()\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i} -> input_ids shape: {batch['input_ids'].shape}\")\n",
    "    if i + 1 >= num_batches:\n",
    "        break\n",
    "t1 = time.time()\n",
    "elapsed = t1 - t0\n",
    "if elapsed > 0:\n",
    "    processed = (i + 1) * 8 * block_size\n",
    "    print(f\"Elapsed: {elapsed:.2f}s — approx processed tokens: {processed} — tokens/sec: {processed/elapsed:.2f}\")\n",
    "else:\n",
    "    print(\"Elapsed time too small to measure throughput\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Small utilities and token statistics\n",
    "\n",
    "We compute some quick streaming statistics like average tokens per example for the first N examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stats(hf_iter, n_examples=1000):\n",
    "    cnt = 0\n",
    "    token_counts = []\n",
    "    t0 = time.time()\n",
    "    for ex in hf_iter:\n",
    "        ids = ex.get(\"input_ids\") if isinstance(ex, dict) else None\n",
    "        if ids is None:\n",
    "            continue\n",
    "        token_counts.append(len(ids))\n",
    "        cnt += 1\n",
    "        if cnt >= n_examples:\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    if token_counts:\n",
    "        return {\n",
    "            \"examples\": cnt,\n",
    "            \"avg_tokens_per_example\": sum(token_counts)/len(token_counts),\n",
    "            \"median_tokens_per_example\": sorted(token_counts)[len(token_counts)//2],\n",
    "            \"time_sec\": t1-t0,\n",
    "            \"examples_per_sec\": cnt/(t1-t0) if (t1-t0)>0 else float('inf')\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "# WARNING: sampling can take time on very large datasets — use a small n when running locally\n",
    "# stats = sample_stats(tokenized_stream, n_examples=200)\n",
    "# print(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Notes for submission\n",
    "\n",
    "- This lab modifies the original Lab 2 by using `openwebtext` (streaming), switching to the\n",
    "  `gpt2` tokenizer, and using a block size of 256 tokens.\n",
    "- The pipeline remains streaming and memory-efficient.\n",
    "- If submitting to a grader that runs the notebook, they should have `datasets` and `transformers` installed.\n",
    "\n",
    "### How to run\n",
    "1. Run all cells in order in Colab / Jupyter / VS Code.\n",
    "2. If data fails to stream (rate limits), consider switching to a smaller dataset or running with HF cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab 2 (modified) notebook ready. Save this file and submit as your lab deliverable.\n"
     ]
    }
   ],
   "source": [
    "# Notebook metadata footer\n",
    "print('Lab 2 (modified) notebook ready. Save this file and submit as your lab deliverable.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
